{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b256a1b-5730-48e6-a5ea-6eb7c8f1ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee716671-9291-4451-8246-26c754d70b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/autodl-tmp'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b9115f-bc61-4ff8-a4af-d14d7a8c8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Path\n",
    "path = \"/root/autodl-tmp/data/aclImdb/train/neg\"\n",
    "# Change the directory\n",
    "os.chdir(path)\n",
    "\n",
    "train_neg = []\n",
    "\n",
    "# Read text File\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        train_neg.append(f.read())\n",
    "  \n",
    "\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path}/{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        read_text_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf50349-34f9-478b-b25b-46839844e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/root/autodl-tmp/data/aclImdb/train/pos\"\n",
    "# Change the directory\n",
    "os.chdir(path)\n",
    "\n",
    "train_pos = []\n",
    "\n",
    "# Read text File\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        train_pos.append(f.read())\n",
    "  \n",
    "\n",
    "# iterate through all file\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path}/{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        read_text_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf1d9d9-013b-41c4-bdf1-98550964a98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n"
     ]
    }
   ],
   "source": [
    "print(len(train_pos)) # number of positive comments\n",
    "print(len(train_neg)) # number of negative comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94edf936-a07b-40b3-9dd8-8e57038b5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = train_neg + train_pos\n",
    "y_train_neg = [0]*len(train_neg)\n",
    "y_train_pos = [1]*len(train_pos)\n",
    "y_train = np.array(y_train_neg + y_train_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4d2fa1-f03c-4c79-94e8-aa99167296c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I think my summary sums it up. I found it inane and stupid. I also saw the ending a mile a way. Everyone is copying that ending anymore when doing a TV/Theater crossover anymore. Sometimes, it's better to let the movie stand alone.<br /><br />Others, its better to forget the movie altogether. This is one of the others....\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_train[12499] # negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c75efada-350c-477d-abb5-53f6765ac27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zentropa has much in common with The Third Man, another noir-like film set among the rubble of postwar Europe. Like TTM, there is much inventive camera work. There is an innocent American who gets emotionally involved with a woman he doesn\\'t really understand, and whose naivety is all the more striking in contrast with the natives.<br /><br />But I\\'d have to say that The Third Man has a more well-crafted storyline. Zentropa is a bit disjointed in this respect. Perhaps this is intentional: it is presented as a dream/nightmare, and making it too coherent would spoil the effect. <br /><br />This movie is unrelentingly grim--\"noir\" in more than one sense; one never sees the sun shine. Grim, but intriguing, and frightening.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_train[12500] # positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10882c63-02c1-4c1e-b39d-0811c43ac742",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = 10000\n",
    "tokenizer = Tokenizer(num_words=vocabulary)\n",
    "tokenizer.fit_on_texts(texts_train) # build dictionary\n",
    "\n",
    "word_index = tokenizer.word_index # word dictionary\n",
    "sequence_train = tokenizer.texts_to_sequences(texts_train) # text to sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7250c4b2-f137-4410-9b62-261e5549f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# align sequences\n",
    "word_num = 20\n",
    "X_train = preprocessing.sequence.pad_sequences(sequence_train, maxlen=word_num) # Pre-padding or removing values from the beginning of the sequence is the default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885de6c-f7c8-484a-8827-e03352d702f4",
   "metadata": {},
   "source": [
    "Preprocessing done (tokenization -> encoding -> alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f3a815-46e7-4f78-a016-8657aab998f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "410e0687-4352-4278-9571-71bc74171312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 20, 8)             80000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-05 15:26:22.833277: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-05 15:26:23.545506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22312 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:3e:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 8\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary, embedding_dim, input_length=word_num))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95064a26-30e6-47c9-a092-1b5cf979bada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),loss='binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a34c2d6f-ed86-4d1e-9adf-fa8df1a39e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 93/625 [===>..........................] - ETA: 0s - loss: 0.6930 - acc: 0.5114 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-05 15:26:25.395588: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 3s 2ms/step - loss: 0.6928 - acc: 0.5103 - val_loss: 0.6922 - val_acc: 0.5212\n",
      "Epoch 2/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.6909 - acc: 0.5459 - val_loss: 0.6912 - val_acc: 0.5392\n",
      "Epoch 3/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.6885 - acc: 0.5812 - val_loss: 0.6896 - val_acc: 0.5606\n",
      "Epoch 4/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.6853 - acc: 0.6166 - val_loss: 0.6872 - val_acc: 0.5840\n",
      "Epoch 5/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.6810 - acc: 0.6535 - val_loss: 0.6835 - val_acc: 0.6062\n",
      "Epoch 6/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.6749 - acc: 0.6838 - val_loss: 0.6782 - val_acc: 0.6346\n",
      "Epoch 7/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.6669 - acc: 0.7046 - val_loss: 0.6708 - val_acc: 0.6582\n",
      "Epoch 8/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.6566 - acc: 0.7194 - val_loss: 0.6614 - val_acc: 0.6768\n",
      "Epoch 9/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.6441 - acc: 0.7286 - val_loss: 0.6500 - val_acc: 0.6900\n",
      "Epoch 10/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.6297 - acc: 0.7360 - val_loss: 0.6372 - val_acc: 0.6982\n",
      "Epoch 11/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.6140 - acc: 0.7412 - val_loss: 0.6234 - val_acc: 0.7046\n",
      "Epoch 12/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.5976 - acc: 0.7463 - val_loss: 0.6094 - val_acc: 0.7110\n",
      "Epoch 13/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.5813 - acc: 0.7517 - val_loss: 0.5957 - val_acc: 0.7144\n",
      "Epoch 14/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.5653 - acc: 0.7569 - val_loss: 0.5826 - val_acc: 0.7186\n",
      "Epoch 15/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.5501 - acc: 0.7606 - val_loss: 0.5705 - val_acc: 0.7226\n",
      "Epoch 16/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.5360 - acc: 0.7659 - val_loss: 0.5594 - val_acc: 0.7284\n",
      "Epoch 17/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.5230 - acc: 0.7701 - val_loss: 0.5496 - val_acc: 0.7334\n",
      "Epoch 18/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.5111 - acc: 0.7743 - val_loss: 0.5407 - val_acc: 0.7368\n",
      "Epoch 19/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.5001 - acc: 0.7786 - val_loss: 0.5328 - val_acc: 0.7396\n",
      "Epoch 20/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4900 - acc: 0.7828 - val_loss: 0.5257 - val_acc: 0.7442\n",
      "Epoch 21/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4807 - acc: 0.7867 - val_loss: 0.5194 - val_acc: 0.7452\n",
      "Epoch 22/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4722 - acc: 0.7903 - val_loss: 0.5139 - val_acc: 0.7470\n",
      "Epoch 23/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4643 - acc: 0.7935 - val_loss: 0.5090 - val_acc: 0.7500\n",
      "Epoch 24/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4571 - acc: 0.7966 - val_loss: 0.5047 - val_acc: 0.7510\n",
      "Epoch 25/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4503 - acc: 0.7998 - val_loss: 0.5008 - val_acc: 0.7534\n",
      "Epoch 26/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4440 - acc: 0.8032 - val_loss: 0.4974 - val_acc: 0.7576\n",
      "Epoch 27/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4381 - acc: 0.8055 - val_loss: 0.4944 - val_acc: 0.7580\n",
      "Epoch 28/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4325 - acc: 0.8079 - val_loss: 0.4917 - val_acc: 0.7590\n",
      "Epoch 29/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4273 - acc: 0.8106 - val_loss: 0.4893 - val_acc: 0.7592\n",
      "Epoch 30/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4223 - acc: 0.8127 - val_loss: 0.4872 - val_acc: 0.7622\n",
      "Epoch 31/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4176 - acc: 0.8154 - val_loss: 0.4854 - val_acc: 0.7632\n",
      "Epoch 32/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4132 - acc: 0.8185 - val_loss: 0.4838 - val_acc: 0.7652\n",
      "Epoch 33/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4090 - acc: 0.8209 - val_loss: 0.4824 - val_acc: 0.7658\n",
      "Epoch 34/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4048 - acc: 0.8232 - val_loss: 0.4812 - val_acc: 0.7654\n",
      "Epoch 35/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.4009 - acc: 0.8242 - val_loss: 0.4801 - val_acc: 0.7662\n",
      "Epoch 36/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3971 - acc: 0.8261 - val_loss: 0.4793 - val_acc: 0.7656\n",
      "Epoch 37/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3934 - acc: 0.8281 - val_loss: 0.4785 - val_acc: 0.7658\n",
      "Epoch 38/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3899 - acc: 0.8306 - val_loss: 0.4779 - val_acc: 0.7656\n",
      "Epoch 39/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3864 - acc: 0.8327 - val_loss: 0.4774 - val_acc: 0.7648\n",
      "Epoch 40/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3831 - acc: 0.8343 - val_loss: 0.4769 - val_acc: 0.7646\n",
      "Epoch 41/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3798 - acc: 0.8360 - val_loss: 0.4767 - val_acc: 0.7642\n",
      "Epoch 42/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3766 - acc: 0.8371 - val_loss: 0.4765 - val_acc: 0.7648\n",
      "Epoch 43/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3735 - acc: 0.8391 - val_loss: 0.4763 - val_acc: 0.7664\n",
      "Epoch 44/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3704 - acc: 0.8397 - val_loss: 0.4763 - val_acc: 0.7666\n",
      "Epoch 45/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3675 - acc: 0.8415 - val_loss: 0.4763 - val_acc: 0.7666\n",
      "Epoch 46/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3645 - acc: 0.8429 - val_loss: 0.4765 - val_acc: 0.7654\n",
      "Epoch 47/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3617 - acc: 0.8441 - val_loss: 0.4767 - val_acc: 0.7658\n",
      "Epoch 48/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3589 - acc: 0.8454 - val_loss: 0.4768 - val_acc: 0.7672\n",
      "Epoch 49/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3561 - acc: 0.8472 - val_loss: 0.4771 - val_acc: 0.7660\n",
      "Epoch 50/50\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.3534 - acc: 0.8496 - val_loss: 0.4773 - val_acc: 0.7668\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, epochs=epochs, batch_size=32, validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4959fb66-97e8-4d29-b2ed-539203a892e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I found this film to be quite an oddity. From the very get go I found it extremely hard to like this movie, and now after a little thinking about it I can pretty much pinpoint the reason why. Jean-Marc Barr, although I love him to bits (I think Zentropa is one of the best movies ever made) is quite miscast here, and although I can't figure for the life of me who would be better, I am sure someone could have taken his place quite easily and make this film work. Everything else is fine, except for the stabs at weak comedy (A Meet The Parents Joke is not really needed, filmmakers!) and I really like Richard E. Grant as the British Major. It just suffers from one thing.. Jean-Marc.\"]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = [texts_train[610]]\n",
    "comments # should be negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a27fe57-e8e3-4167-9d97-2753c6ff93d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35534427]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_prediction = tokenizer.texts_to_sequences(comments) # input should be list of sequences\n",
    "xhat = preprocessing.sequence.pad_sequences(sequence_prediction, maxlen=word_num)\n",
    "model.predict(xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202c8da-6088-45b5-a031-0d3aa8c2ef23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbca96f-4d97-4ac0-937f-2dfb3b956d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
